## Projeto_Engenharia_De_Dados

#### Equipe: Weslaine Santana e Jo√£o Fontanella

# üî• Projeto de Demonstra√ß√£o: Apache Spark com Delta Lake e Iceberg

Este reposit√≥rio apresenta um ambiente local utilizando Apache Spark (via PySpark), integrando os formatos Delta Lake e Apache Iceberg. O foco √© explorar a manipula√ß√£o de dados em formatos otimizados, demonstrando comandos como `INSERT`, `UPDATE` e `DELETE` em ambos os formatos.

O projeto foi estruturado seguindo boas pr√°ticas para Engenharia de Dados e inclui:
- Ambiente Python isolado com **UV**
- Notebooks Jupyter com exemplos pr√°ticos
- Tabelas simuladas com modelo ER, DDL e dados p√∫blicos
- Documenta√ß√£o com **MKDocs**

---

## üì¶ Configura√ß√£o do Ambiente

> √â recomendado o uso do **UV** para garantir o isolamento e controle do ambiente.

### ‚úÖ Instala√ß√£o do ambiente com UV:
```bash
uv init
uv venv
source .venv/bin/activate
uv add pyspark==3.5.3 delta-spark==3.2.0 jupyterlab ipykernel
```

### üí° Alternativas:
- **Com pip**:
```bash
pip install -r requirements.txt
```

- **Com poetry** (caso esteja usando `pyproject.toml`):
```bash
poetry install
```

### üìå Ativando o ambiente:
```bash
source .venv/bin/activate
```

---

## üß™ Execu√ß√£o dos Notebooks

Certifique-se de selecionar o ambiente virtual como kernel no JupyterLab:

```bash
jupyter lab
```

Os notebooks principais:
- `notebooks/spark-delta.ipynb`
- `notebooks/spark-iceberg.ipynb`

Eles cont√™m os exemplos de cria√ß√£o e manipula√ß√£o de tabelas Delta e Iceberg.

---

## ‚öôÔ∏è Requisitos de Sistema

- **Python 3.10 ou superior**
- **JDK 8, 11 ou 17** (n√£o use JDK 21)
- **Apache Spark 3.5.x**
- **Ambiente Linux/Mac ou Windows com ajustes**

---

## ‚ö†Ô∏è Problemas Comuns e Solu√ß√µes (Windows)

### Erro com `JAVA_HOME`:
> Certifique-se de que o Java est√° corretamente instalado e configurado.

Baixe o JDK 11:
- https://adoptium.net/temurin/releases/?version=11

Configure as vari√°veis de ambiente:
```powershell
$env:JAVA_HOME="C:\Program Files\Java\jdk-11"
$env:PATH="$env:JAVA_HOME\bin;$env:PATH"
```

---

### Erro: `HADOOP_HOME and hadoop.home.dir are unset`

**Causa**: Spark no Windows tenta acessar recursos do Hadoop ausentes.

### Solu√ß√£o:
1. Baixe o [winutils.exe](https://github.com/cdarlint/winutils) (Hadoop 2.7.1 recomendado).
2. Coloque em: `C:\hadoop\bin\winutils.exe`
3. Configure as vari√°veis:
```python
import os

os.environ['HADOOP_HOME'] = 'C:\\hadoop'
os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')
```

---

## üìä Exemplo de Inicializa√ß√£o do Spark com Iceberg

```python
from pyspark.sql import SparkSession
import os

os.environ['HADOOP_HOME'] = 'C:\\hadoop'
os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')

spark = SparkSession.builder \
    .appName("IcebergExample") \
    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1') \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.local.type", "hadoop") \
    .config("spark.sql.catalog.local.warehouse", "spark-warehouse/iceberg") \
    .getOrCreate()
```

---
